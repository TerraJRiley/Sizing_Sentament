{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Big General Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Processing Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Basic Model Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Scoring Imports\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Neural Net Imports\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Recycled Imports\n",
    "#import math\n",
    "#import nltk\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runway_top    = pd.read_csv(\"./data/renttherunway_first.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "\n",
    "df_runway_bottom = pd.read_csv(\"./data/renttherunway_last.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "\n",
    "df_modcloth = pd.read_json('./data/modcloth_final_data.json', lines = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268542</th>\n",
       "      <td>Cute jacket!</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268543</th>\n",
       "      <td>It's a beautiful jacket. I love how it's knit ...</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268544</th>\n",
       "      <td>I love this blazer. It is a great office piece...</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268545</th>\n",
       "      <td>I love this blazer!! I wore it yesterday and g...</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268546</th>\n",
       "      <td>I love this piece. I'm really happy with it!</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review_text    fit\n",
       "268542                                       Cute jacket!    fit\n",
       "268543  It's a beautiful jacket. I love how it's knit ...  small\n",
       "268544  I love this blazer. It is a great office piece...    fit\n",
       "268545  I love this blazer!! I wore it yesterday and g...    fit\n",
       "268546       I love this piece. I'm really happy with it!    fit"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.concat([df_runway_top[[\"review_text\", \"fit\"]],\n",
    "                    df_runway_bottom[[\"review_text\", \"fit\"]],\n",
    "                    df_modcloth[[\"review_text\", \"fit\"]]\n",
    "                   ]).dropna().reset_index(drop = True)\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"./data/temp_all_reviews.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "df_modcloth = pd.read_json('./data/modcloth_final_data.json', lines = True)\n",
    "print(\"Prior to DF Cleaning:\")\n",
    "print(df_modcloth[\"fit\"].value_counts()) # Prior to cleaning\n",
    "\n",
    "# Filter for just Large and Small reviews\n",
    "df_modcloth = df_modcloth[\n",
    "    (df_modcloth[\"fit\"] == \"large\") | \n",
    "    (df_modcloth[\"fit\"] == \"small\")]\n",
    "\n",
    "# Map Fit for Better Machine Learning\n",
    "df_modcloth[\"fit\"] = df_modcloth[\"fit\"].map({\"small\": 0, \"large\": 1})\n",
    "\n",
    "# Removing NA's & Resetting Index\n",
    "df_modcloth = df_modcloth[[\"review_text\", \"fit\"]].dropna().reset_index(drop = True)\n",
    "X = df_modcloth[\"review_text\"]\n",
    "y = df_modcloth[\"fit\"]\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"Post DF Cleaning\")\n",
    "print(df_modcloth[\"fit\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all[\"review_text\"]\n",
    "y = df_all[\"fit\"].map({\"small\": -1, \"fit\":0, \"large\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(268547, 9254)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Bag-of-Words Processing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word) for word \n",
    "                     in tokens if len(word) > 1 and not word in stop_words])\n",
    "tfidf = TfidfVectorizer(analyzer = \"word\",\n",
    "                       min_df = 7,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words = 'english')\n",
    "\n",
    "# Fit transform features AKA The Documents\n",
    "X = tfidf.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7508\\2798668547.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Print Results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_rf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Confusion Matrix:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_mat_rf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc_rf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize and train the model\n",
    "model_rf = RandomForestClassifier(random_state=1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "conf_mat_rf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(\"Accuracy: \", acc_rf)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7787562837460436\n",
      "Confusion Matrix:\n",
      " [[ 1645  5607   145]\n",
      " [  295 38325   270]\n",
      " [  119  5447  1857]]\n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1916,  5322,   159],\n",
       "       [  454, 38059,   377],\n",
       "       [  176,  5322,  1925]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.780320238316887\n",
      "Confusion Matrix:\n",
      "[[ 2653  4342   402]\n",
      " [ 1315 36382  1193]\n",
      " [  372  4175  2876]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression model\n",
    "model_lr = LogisticRegression(random_state=1, max_iter = 1000)\n",
    "# Train the model\n",
    "model_lr.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "# Calculate accuracy and confusion matrix for Logistic Regression\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "conf_mat_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_mat_lr}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.757680134053249\n",
      "Confusion Matrix:\n",
      "[[ 3297  3453   647]\n",
      " [ 2579 34148  2163]\n",
      " [  722  3451  3250]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Naive Bayes model\n",
    "model_nb = MultinomialNB()\n",
    "# Train the model\n",
    "model_nb.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "# Calculate accuracy and confusion matrix for Naive Bayes\n",
    "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "conf_mat_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Naive Bayes Accuracy: {acc_nb}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_mat_nb}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Support Vector Machine model\n",
    "model_svm = SVC(random_state=1)\n",
    "# Train the model\n",
    "model_svm.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "# Calculate accuracy and confusion matrix for Support Vector Machine\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "conf_mat_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Support Vector Machine Accuracy: {acc_svm}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_mat_svm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7508\\4212859328.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Fit the grid search to the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mgrid_search_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Print the best parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    849\u001b[0m                     )\n\u001b[0;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[1;32m--> 851\u001b[1;33m                         \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m                     )\n\u001b[0;32m    853\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1952\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1594\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1595\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1697\u001b[0m             \u001b[1;31m# worker traceback.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1699\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1700\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1732\u001b[0m         \u001b[1;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1733\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1734\u001b[1;33m             \u001b[0merror_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[1;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;31m# be returned.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "#    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "#    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_rf = GridSearchCV(estimator=model_rf, param_grid=param_grid_rf, \n",
    "                              cv=3, n_jobs=2, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", grid_search_rf.best_params_)\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "model_rf_best = grid_search_rf.best_estimator_\n",
    "y_pred_best = model_rf_best.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "acc_best = accuracy_score(y_test, y_pred_best)\n",
    "conf_mat_best = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Print Results\n",
    "print(\"Best Accuracy: \", acc_best)\n",
    "print(\"Best Confusion Matrix:\\n\", conf_mat_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Initialize other classifiers to include in the ensemble\n",
    "model_lr = LogisticRegression(random_state=1)\n",
    "model_nb = MultinomialNB()\n",
    "model_svc = SVC(probability=True, random_state=1)\n",
    "\n",
    "# Create a list of tuples with classifier name and classifier object\n",
    "classifiers = [\n",
    "    ('rf', model_rf_best),\n",
    "    ('lr', model_lr),\n",
    "    ('nb', model_nb),\n",
    "    ('svc', model_svc)\n",
    "]\n",
    "\n",
    "# Initialize the VotingClassifier with soft voting\n",
    "ensemble_model = VotingClassifier(estimators=classifiers, voting='soft')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "acc_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "conf_mat_ensemble = confusion_matrix(y_test, y_pred_ensemble)\n",
    "\n",
    "# Print results\n",
    "print(\"Ensemble Accuracy: \", acc_ensemble)\n",
    "print(\"Ensemble Confusion Matrix:\\n\", conf_mat_ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27440\\1885792820.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Assuming 'vec' is the CountVectorizer instance you used to fit your training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_text_vectorized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"this was super duper short\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Now you can predict using your trained Random Forest classifier 'clf_rf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_text_vectorized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1374\u001b[1;33m                 \u001b[1;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m             )\n\u001b[0;32m   1376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "# Assuming 'vec' is the CountVectorizer instance you used to fit your training data\n",
    "new_text_vectorized = cvec.transform(\"this was super duper short\")\n",
    "\n",
    "# Now you can predict using your trained Random Forest classifier 'model_rf'\n",
    "prediction = model_rf.predict(new_text_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Peridot of Earth\\anaconda3\\envs\\NNEnv\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 16660 samples, validate on 4166 samples\n",
      "Epoch 1/5\n",
      "16660/16660 [==============================] - 4s 222us/sample - loss: 0.5334 - acc: 0.7327 - val_loss: 0.4909 - val_acc: 0.7439\n",
      "Epoch 2/5\n",
      "16660/16660 [==============================] - 4s 214us/sample - loss: 0.4205 - acc: 0.7980 - val_loss: 0.4890 - val_acc: 0.7475\n",
      "Epoch 3/5\n",
      "16660/16660 [==============================] - 3s 200us/sample - loss: 0.3583 - acc: 0.8315 - val_loss: 0.5057 - val_acc: 0.7513\n",
      "Epoch 4/5\n",
      "16660/16660 [==============================] - 3s 207us/sample - loss: 0.3006 - acc: 0.8646 - val_loss: 0.5298 - val_acc: 0.7518\n",
      "Epoch 5/5\n",
      "16660/16660 [==============================] - 3s 201us/sample - loss: 0.2507 - acc: 0.8882 - val_loss: 0.5635 - val_acc: 0.7523\n",
      "5207/5207 [==============================] - 0s 78us/sample - loss: 0.5415 - acc: 0.7678\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert data to float32\n",
    "X_train_tf = X_train.values.astype('float32')\n",
    "X_test_tf = X_test.values.astype('float32')\n",
    "y_train_tf = y_train.values.astype('float32')\n",
    "y_test_tf = y_test.values.astype('float32')\n",
    "\n",
    "# Build and compile the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_tf.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_tf, y_train_tf, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = model.evaluate(X_test_tf, y_test_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X, series_fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For new text prediction\n",
    "new_text = \"this top is too small\"\n",
    "new_text_processed = vec.transform([new_text])\n",
    "prediction_rf = model_rf.predict(new_text_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_rf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted fit for the review is: small\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code for loading data, preprocessing, and model training)\n",
    "\n",
    "# Define the function to interpret the prediction\n",
    "def get_fit_label(prediction):\n",
    "    label_map = {0: \"small\", 1: \"large\"}\n",
    "    return label_map.get(prediction[0], \"Unknown\")\n",
    "\n",
    "# Example usage\n",
    "new_text = \"this top is too small\"\n",
    "new_text_processed = vec.transform([new_text])\n",
    "prediction_rf = model_rf.predict(new_text_processed)\n",
    "\n",
    "# Get and print the prediction\n",
    "predicted_label = get_fit_label(prediction_rf)\n",
    "print(f\"The predicted fit for the review is: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scaled fit for the review is: -0.28\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with probability estimates\n",
    "new_text = \"kind of small but also a bit wide in other places\"\n",
    "new_text_processed = vec.transform([new_text])\n",
    "prediction_prob = model_rf.predict_proba(new_text_processed)\n",
    "\n",
    "# Scale the probability of the 'large' class from [0, 1] to [-1, 1]\n",
    "# 'large' is assumed to be the second class (index 1)\n",
    "scaled_prediction = prediction_prob[0][1] * 2 - 1  # This converts the scale\n",
    "\n",
    "print(f\"The scaled fit for the review is: {scaled_prediction:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64, 0.36]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recycling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Model Prep\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, Normalizer#, Binarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB#, BernoulliNB, GaussianNB\n",
    "#from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = PolynomialFeatures().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "X = Normalizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = PCA(n_components= \n",
    "        components_or_alteredfeatures_in_model).fit_transform(X)  # Is poly features wrapped up in PCA already???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarianceThreshold # Remove features that have the same value in more that XX% of the column\n",
    "# Used to cut out potentially high variance risk columns by identifying the ones with, for example: \n",
    "# 10% == 1, and 90% == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping (for Model Consumption)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify = y, random_state=42)\n",
    "\n",
    "# Model Prep\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, Normalizer#, Binarizer\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "X = Normalizer().fit_transform(X)\n",
    "\n",
    "pd.DataFrame(X_scaled, columns=features).head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature Engineering (New Features)\n",
    "\n",
    "\n",
    "# Bootstrapping (Not our code, copied from DSI)\n",
    "\n",
    "def bootstrap_sample(values, statistic, num_samples): \n",
    "    bootstrap_statistics = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        subset = np.random.choice(values, size = 1000, replace =True)\n",
    "        stat = statistic(subset)\n",
    "        bootstrap_statistics.append(stat)\n",
    "        \n",
    "    return bootstrap_statistics \n",
    "\n",
    "#==============================================\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Poly Features\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "features = ['col1', 'col2', 'col3']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly[:5, :]\n",
    "\n",
    "poly.get_feature_names(features)\n",
    "\n",
    "pd.DataFrame(X_poly, columns=poly.get_feature_names(features)).head()\n",
    "\n",
    "# Copied over from the PCA lession\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "pca = pca.fit(X_train_scaled, y_train)\n",
    "\n",
    "pca_train = pca.transform(X_train)\n",
    "pca_test = pca.transform(X_test_scaled)\n",
    "\n",
    "#==============================================\n",
    "\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "#print(explained_variance)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "for i in cumulative_explained_variance:\n",
    "    print(i)\n",
    "\n",
    "#==============================================\n",
    "X = PCA(n_components= \n",
    "        components_or_alteredfeatures_in_model).fit_transform(X)  # Is poly features wrapped up in PCA already???\n",
    "\n",
    "VarianceThreshold # Remove features that have the same value in more that XX% of the column\n",
    "# Used to cut out potentially high variance risk columns by identifying the ones with, for example: \n",
    "# 10% == 1, and 90% == 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#====================================\n",
    "# Random Bullcrap\n",
    "\n",
    "\n",
    "\n",
    "# Computing Standard Deviation with list comprehension & no loops (don't know why it's \"pop\"\n",
    "def pop_std(the_column):\n",
    "    return round(((sum([(x - the_column.mean())**2 for x in the_column]))/len(the_column))**(1/2))\n",
    "pop_std(test_df['Sat_Total'])\n",
    "\n",
    "# Alternative remaping code\n",
    "df['column'] = [float(x.replace('example','')) for x in df['column']]\n",
    "\n",
    "\n",
    "# Using .items() to make a column into a dictionary\n",
    "dict = {}\n",
    "for key, value in df['column'].items():\n",
    "    dict[key] = value\n",
    "\n",
    "# Scales\n",
    "# Scaled\n",
    "tv_mean = df['TV'].mean()\n",
    "((df['TV'] - tv_mean) / df['TV'].std(ddof=0)).head() #Manually\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "pd.scatter_matrix(df) # Alt sns.pairplot?\n",
    "df.mean().sort_values().plot(style = '.') # Good for organizing things when otherwise random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prep & metric imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, Normalizer\n",
    "#, Binarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Regression model imports\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "#==========================================\n",
    "\n",
    "\n",
    "\n",
    "def test_model(df, test_model, features, target = 'Target'):\n",
    "    model = test_model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[features], df[target]) \n",
    "    model = model.fit(X_train,y_train)\n",
    "\n",
    "    print('Train:', model.score(X_train,y_train))\n",
    "    print('Test: ',  model.score(X_test,y_test))\n",
    "    return\n",
    "\n",
    "\n",
    "# Simple Model Forms\n",
    "\n",
    "features = ['columns_etc']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "model = Model(**params).fit(Xtrain,ytrain)\n",
    "model.score(Xtrain,ytrain)\n",
    "model.score(Xtest,ytest)\n",
    "\n",
    "#======================================\n",
    "\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, results_dataframe, save = True, \n",
    "              rando_state = 76, is_neural_network = False, \n",
    "              NN_epochs = 10, NN_batch_size = 32, NN_verbose = 0):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = rando_state)\n",
    "\n",
    "model      = model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test  = model.predict(X_test)\n",
    "dict_model = {'model' : re.findall(r'^[^@]+\\(', str(model))[0].strip(\"(\"),\n",
    "         'parameters' : model.get_params()}\n",
    "        \n",
    "\n",
    "    \n",
    "    # Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    #dict_model['train_Median_error'] = metrics.median_absolute_error(y_train, pred_train)\n",
    "    #dict_model['train_R_squared']    = metrics.r2_score(y_train, pred_train)\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    #dict_model['test_Median_error']  = metrics.median_absolute_error(y_test, pred_test)\n",
    "    #dict_model['test_R_squared']     = metrics.r2_score(y_test, pred_test)\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE : ' + str(dict_model['train_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['train_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['train_R_squared']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE : ' + str(dict_model['test_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['test_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['test_R_squared']))\n",
    "    \n",
    "    # Saving current results\n",
    "    results_dataframe = results_dataframe.append(dict_model, ignore_index=True)\n",
    "    if save == True:\n",
    "        results_dataframe.to_csv('./data/modeling_results_' + datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "                          , index = False)\n",
    "    \n",
    "    return results_dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Regressors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "#, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "scaled_df = pd.DataFrame(StandardScaler().fit_transform(df),\n",
    "                         columns = df.columns)\n",
    "scaled_df.head()\n",
    "\n",
    "X = scaled_df.drop(columns = ['target'])\n",
    "y = scaled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "lin_reg    = LinearRegression().fit(X_train, y_train)\n",
    "knn_reg    = KNeighborsRegressor().fit(X_train, y_train)\n",
    "cart_reg   = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "bag_reg    = BaggingRegressor().fit(X_train, y_train)\n",
    "randof_reg = RandomForestRegressor().fit(X_train, y_train)\n",
    "ada_reg    = AdaBoostRegressor().fit(X_train, y_train)\n",
    "SV_reg     = SVR().fit(X_train, y_train)\n",
    "models     = [\n",
    "    lin_reg,\n",
    "    knn_reg,\n",
    "    cart_reg,\n",
    "    bag_reg,\n",
    "    randof_reg,\n",
    "    ada_reg,\n",
    "    SV_reg    \n",
    "]\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Classifications:\n",
    "X = scaled_df.drop(columns = ['e401k', 'p401k'])\n",
    "y = [1 if scaled_df['e401k'][i] > 0 else 0 for i in range(scaled_df.shape[0])]\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "LogisticRegression().fit(X_train, y_train)\n",
    "KNeighborsClassifier().fit(X_train, y_train)\n",
    "DecisionTreeClassifier().fit(X_train, y_train)\n",
    "BaggingClassifier().fit(X_train, y_train)\n",
    "RandomForestClassifier().fit(X_train, y_train)\n",
    "AdaBoostClassifier().fit(X_train, y_train)\n",
    "SVC().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Multi NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "model = nb.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "model.score(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "X = df[[]]\n",
    "y = df[]\n",
    "df_results = run_model(XGBRegressor(objective='reg:squarederror'), X, y, df_results)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "xgb_regressor=XGBRegressor(max_depth=7, \n",
    "                           n_estimators=500, \n",
    "                           objective=\"reg:linear\", \n",
    "                           min_child_weight = 6,\n",
    "                           subsample = 0.87,\n",
    "                           colsample_bytree = 0.50,\n",
    "                           scale_pos_weight = 1.0,                       \n",
    "                           learning_rate=0.1)\n",
    "\n",
    "xgb_regressor.fit(X_train, y_log, eval_metric=RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Needed:\n",
    " 1.) Imports\n",
    " 2.) Classification Metrics\n",
    " #==================================\n",
    "\n",
    "# For general Regression.\n",
    "def RMSE(y_real, y_hat):  # Goal=0, Avg Distance\n",
    "    return np.sqrt(metrics.mean_squared_error(y_real, y_hat))\n",
    "\n",
    "#==================================\n",
    "# For Neural Network\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#====================================\n",
    "# All Other Important Regression Metrics\n",
    "\n",
    "def Reg_metrics(y_real, y_hat):\n",
    "    print('RMSE            :',np.sqrt(metrics.mean_squared_error(y_real, y_hat)))   # Goal=0, Avg Distance\n",
    "    print('Median Abs Error:',metrics.median_absolute_error(y_real,y_hat))          # Goal=0, Median Distance\n",
    "    print('R Squared       :',metrics.r2_score(y_real, y_hat))          # Goal=1, Percent model can explain\n",
    "    return\n",
    "\n",
    "#====================================\n",
    "\n",
    "\n",
    "def rmse_score(model, \n",
    "               X_train = X_train,\n",
    "               X_test = X_test,\n",
    "               y_train = y_train,\n",
    "               y_test = y_test):\n",
    "    train_score = mean_squared_error(y_true = y_train,\n",
    "                                     y_pred = model.predict(X_train)) ** 0.5\n",
    "    test_score = mean_squared_error(y_true = y_test,\n",
    "                                    y_pred = model.predict(X_test)) ** 0.5\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return\n",
    "\n",
    "#====================================\n",
    "def f1_score(model, \n",
    "               X_train = Xr_train,\n",
    "               X_test = Xr_test,\n",
    "               y_train = yr_train,\n",
    "               y_test = yr_test):\n",
    "    train_score = f1_score(y_train,\n",
    "                           model.predict(X_train))\n",
    "    test_score  = f1_score(y_test,\n",
    "                           model.predict(X_test))\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return\n",
    "\n",
    "#====================================\n",
    "def run_model(model, X_train, X_test, y_train, y_test, results_dataframe, save = True, \n",
    "              rando_state = 76, is_neural_network = False, \n",
    "              NN_epochs = 10, NN_batch_size = 32, NN_verbose = 0):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = rando_state)\n",
    "\n",
    "model      = model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test  = model.predict(X_test)\n",
    "dict_model = {'model' : re.findall(r'^[^@]+\\(', str(model))[0].strip(\"(\"),\n",
    "         'parameters' : model.get_params()}\n",
    "        \n",
    "\n",
    "    \n",
    "    # Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE : ' + str(dict_model['train_RMSE']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE : ' + str(dict_model['test_RMSE']))\n",
    "    \n",
    "    # Saving current results\n",
    "    results_dataframe = results_dataframe.append(dict_model, ignore_index=True)\n",
    "    if save == True:\n",
    "        results_dataframe.to_csv('./data/modeling_results_' + datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "                          , index = False)\n",
    "    \n",
    "    return results_dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#====================================\n",
    "\n",
    "# Alternmative Measures for above Function\n",
    "# Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    #dict_model['train_Median_error'] = metrics.median_absolute_error(y_train, pred_train)\n",
    "    #dict_model['train_R_squared']    = metrics.r2_score(y_train, pred_train)\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    #dict_model['test_Median_error']  = metrics.median_absolute_error(y_test, pred_test)\n",
    "    #dict_model['test_R_squared']     = metrics.r2_score(y_test, pred_test)\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE : ' + str(dict_model['train_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['train_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['train_R_squared']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE : ' + str(dict_model['test_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['test_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['test_R_squared']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Unsupervised Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(the_X,the_kmodel.labels_) # -1 = Bad 0 = Meh 1 = Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "# DBScan\n",
    "#model = DBSCAN(eps = 3, min_samples = 3)\n",
    "#test_df['y_hat'] = model.fit_predict(X)\n",
    "\n",
    "# K Means\n",
    "#k_model = KMeans(n_clusters=3)\n",
    "#model = k_model.fit(df[['x_axis', 'y_axis']])\n",
    "\n",
    "# Visualizing based on 2 columns\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.scatter(the_df['x'],the_df['y'], c = the_df['DBS_y'], alpha=0.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(the_kmodel.cluster_centers_, columns = ['x_axis', 'y_axis'])\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clusters(df,\n",
    "                     n_clusters_kmeans  = 3,\n",
    "                     n_clusters_agg     = 3,\n",
    "                     eps_dbscan         = 3,\n",
    "                     min_samples_dbscan = 3):\n",
    "    \n",
    "    fig,ax  = plt.subplots(nrows = 1, ncols = 4, figsize=(24, 7)) # Init graph backgrounds & their positions\n",
    "    kmeans  = KMeans(n_clusters = n_clusters_kmeans)\n",
    "    agclus  = AgglomerativeClustering(n_clusters = n_clusters_agg)\n",
    "    dbscan  = DBSCAN(eps = eps_dbscan, min_samples = min_samples_dbscan)\n",
    "    model_tuples = [(False,  0, 'label'),\n",
    "                    (kmeans, 1, 'K_Means'), \n",
    "                    (agclus, 2, 'Agg_clust'), \n",
    "                    (dbscan, 3, 'DBScan')]\n",
    "\n",
    "    for tup in model_tuples:\n",
    "        # [0] = Model\n",
    "        # [1] = Axis Number\n",
    "        # [2] = label string\n",
    "        if tup[0] == False:\n",
    "            pass\n",
    "        else:\n",
    "            model = tup[0].fit(df.iloc[:,0:2])\n",
    "            df[tup[2]] = model.labels_\n",
    "        colors = plt.cm.Paired(np.linspace(0, 1, len(df[tup[2]].unique())))\n",
    "        for label, color in zip(df[tup[2]].unique(), colors):\n",
    "            X_  = df[df[tup[2]] == label]\n",
    "            ax[tup[1]].scatter(X_.iloc[:, 0], X_.iloc[:, 1], s = 70,\n",
    "                            color = color, label = label, alpha = .9)\n",
    "\n",
    "        ax[tup[1]].set_title(tup[2], fontsize=30, color = 'Black')\n",
    "        ax[tup[1]].legend(loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['model', 'features', 'parameters',\n",
    "    'train_RMSE', 'train_Median_error', 'train_R_squared',\n",
    "    'test_RMSE',  'test_Median_error',  'test_R_squared', 'time'])\n",
    "\n",
    "def run_model(model, X, y, results_dataframe, save = True, \n",
    "              rando_state = 76, is_neural_network = False, \n",
    "              NN_epochs = 10, NN_batch_size = 32, NN_verbose = 0):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = rando_state)\n",
    "\n",
    "    if is_neural_network == False:\n",
    "        # Performing normal running of model.\n",
    "        model      = model.fit(X_train, y_train)\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_test  = model.predict(X_test)\n",
    "        dict_model = {'model' : re.findall(r'^[^@]+\\(', str(model))[0].strip(\"(\"),\n",
    "                 'parameters' : model.get_params()}\n",
    "        \n",
    "    elif is_neural_network == True:\n",
    "        # Running model for Neural Networks\n",
    "        results = model.fit(X_train, y_train, \n",
    "                    epochs= NN_epochs, \n",
    "                    batch_size = NN_batch_size,\n",
    "                    verbose = NN_verbose,\n",
    "                    validation_data = (X_test,y_test))\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_test  = model.predict(X_test)\n",
    "        dict_model = {'model' : \"Neural Network\",\n",
    "                 'parameters' : {'layers' : [layer.get_config()['units'] for layer in results.model.layers],\n",
    "                                 'batch_size' : NN_batch_size,\n",
    "                                 'epochs'     : NN_epochs}}\n",
    "\n",
    "    else:\n",
    "        print(\"A non-boolian value was passed to is_neural_network.  This is an error.\")\n",
    "        return\n",
    "    \n",
    "    # Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    dict_model['train_Median_error'] = metrics.median_absolute_error(y_train, pred_train)\n",
    "    dict_model['train_R_squared']    = metrics.r2_score(y_train, pred_train)\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    dict_model['test_Median_error']  = metrics.median_absolute_error(y_test, pred_test)\n",
    "    dict_model['test_R_squared']     = metrics.r2_score(y_test, pred_test)\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE             : ' + str(dict_model['train_RMSE']))\n",
    "    print('Median Abs Error : ' + str(dict_model['train_Median_error']))\n",
    "    print('R Squared        : ' + str(dict_model['train_R_squared']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE             : ' + str(dict_model['test_RMSE']))\n",
    "    print('Median Abs Error : ' + str(dict_model['test_Median_error']))\n",
    "    print('R Squared        : ' + str(dict_model['test_R_squared']))\n",
    "    \n",
    "    # Saving current results\n",
    "    results_dataframe = results_dataframe.append(dict_model, ignore_index=True)\n",
    "    if save == True:\n",
    "        results_dataframe.to_csv('./data/modeling_results_' + datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "                          , index = False)\n",
    "    \n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[features]]\n",
    "y = df[target]\n",
    "df_results = run_model(LogisticRegression(), X, y, df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = run_model(KNeighborsClassifier(), X, y, df_results)\n",
    "df_results = run_model(DecisionTreeClassifier(), X, y, df_results)\n",
    "df_results = run_model(RandomForestClassifier(), X, y, df_results)\n",
    "df_results = run_model(AdaBoostClassifier(), X, y, df_results)\n",
    "df_results = run_model(ExtraTreesClassifier(), X, y, df_results)\n",
    "df_results = run_model(GradientBoostingClassifier(), X, y, df_results)\n",
    "df_results = run_model(SVR(), X, y, df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'param':[list_of_options]\n",
    "}\n",
    "Model_gridsearch = GridSearchCV(Model(), params, cv=5, verbose=1, n_jobs=2,)\n",
    "\n",
    "\n",
    "df_results = run_model(Model_gridsearch, X, y, df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_mod_test = pd.read_csv('./data/mod_test.csv')\n",
    "df_mod_test_target = pd.read_csv('./data/mod_test_target.csv')\n",
    "\n",
    "# Drop the 'Unnamed: 0' columns\n",
    "df_mod_test = df_mod_test.drop(columns=['Unnamed: 0'])\n",
    "df_mod_test_target = df_mod_test_target.drop(columns=['Unnamed: 0'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This hugged in all the right places! It was a ...</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>fit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  fit\n",
       "0  An adorable romper! Belt and zipper were a lit...  fit\n",
       "1  I rented this dress for a photo shoot. The the...  fit\n",
       "2  This hugged in all the right places! It was a ...  fit\n",
       "3  I rented this for my company's black tie award...  fit\n",
       "4  I have always been petite in my upper body and...  fit"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_runway_top    = pd.read_csv(\"./data/renttherunway_first.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df_runway_top    = df_runway_top[[\"review_text\", \"fit\"]].dropna().reset_index(drop = True)\n",
    "df_runway_top.head()\n",
    "\n",
    "df_runway_bottom = pd.read_csv(\"./data/renttherunway_last.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df_runway_bottom = df_runway_bottom[[\"review_text\", \"fit\"]].dropna().reset_index(drop = True)\n",
    "df_runway_bottom.head()\n",
    "\n",
    "df_modcloth = pd.read_json('./data/modcloth_final_data.json', lines = True)\n",
    "df_modcloth = df_modcloth[[\"review_text\", \"fit\"]].dropna().reset_index(drop = True)\n",
    "df_modcloth.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNEnv",
   "language": "python",
   "name": "nnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
