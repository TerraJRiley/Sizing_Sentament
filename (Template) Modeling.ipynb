{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Prep\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, Normalizer#, Binarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB#, BernoulliNB, GaussianNB\n",
    "#from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = PolynomialFeatures().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "X = Normalizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = PCA(n_components= \n",
    "        components_or_alteredfeatures_in_model).fit_transform(X)  # Is poly features wrapped up in PCA already???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarianceThreshold # Remove features that have the same value in more that XX% of the column\n",
    "# Used to cut out potentially high variance risk columns by identifying the ones with, for example: \n",
    "# 10% == 1, and 90% == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping (for Model Consumption)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify = y, random_state=42)\n",
    "\n",
    "# Model Prep\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, Normalizer#, Binarizer\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "X = Normalizer().fit_transform(X)\n",
    "\n",
    "pd.DataFrame(X_scaled, columns=features).head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature Engineering (New Features)\n",
    "\n",
    "\n",
    "# Bootstrapping (Not our code, copied from DSI)\n",
    "\n",
    "def bootstrap_sample(values, statistic, num_samples): \n",
    "    bootstrap_statistics = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        subset = np.random.choice(values, size = 1000, replace =True)\n",
    "        stat = statistic(subset)\n",
    "        bootstrap_statistics.append(stat)\n",
    "        \n",
    "    return bootstrap_statistics \n",
    "\n",
    "#==============================================\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Poly Features\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "features = ['col1', 'col2', 'col3']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly[:5, :]\n",
    "\n",
    "poly.get_feature_names(features)\n",
    "\n",
    "pd.DataFrame(X_poly, columns=poly.get_feature_names(features)).head()\n",
    "\n",
    "# Copied over from the PCA lession\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "pca = pca.fit(X_train_scaled, y_train)\n",
    "\n",
    "pca_train = pca.transform(X_train)\n",
    "pca_test = pca.transform(X_test_scaled)\n",
    "\n",
    "#==============================================\n",
    "\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "#print(explained_variance)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "for i in cumulative_explained_variance:\n",
    "    print(i)\n",
    "\n",
    "#==============================================\n",
    "X = PCA(n_components= \n",
    "        components_or_alteredfeatures_in_model).fit_transform(X)  # Is poly features wrapped up in PCA already???\n",
    "\n",
    "VarianceThreshold # Remove features that have the same value in more that XX% of the column\n",
    "# Used to cut out potentially high variance risk columns by identifying the ones with, for example: \n",
    "# 10% == 1, and 90% == 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#====================================\n",
    "# Random Bullcrap\n",
    "\n",
    "\n",
    "\n",
    "# Computing Standard Deviation with list comprehension & no loops (don't know why it's \"pop\"\n",
    "def pop_std(the_column):\n",
    "    return round(((sum([(x - the_column.mean())**2 for x in the_column]))/len(the_column))**(1/2))\n",
    "pop_std(test_df['Sat_Total'])\n",
    "\n",
    "# Alternative remaping code\n",
    "df['column'] = [float(x.replace('example','')) for x in df['column']]\n",
    "\n",
    "\n",
    "# Using .items() to make a column into a dictionary\n",
    "dict = {}\n",
    "for key, value in df['column'].items():\n",
    "    dict[key] = value\n",
    "\n",
    "# Scales\n",
    "# Scaled\n",
    "tv_mean = df['TV'].mean()\n",
    "((df['TV'] - tv_mean) / df['TV'].std(ddof=0)).head() #Manually\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "pd.scatter_matrix(df) # Alt sns.pairplot?\n",
    "df.mean().sort_values().plot(style = '.') # Good for organizing things when otherwise random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prep & metric imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, Normalizer\n",
    "#, Binarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Regression model imports\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "#==========================================\n",
    "\n",
    "\n",
    "\n",
    "def test_model(df, test_model, features, target = 'Target'):\n",
    "    model = test_model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[features], df[target]) \n",
    "    model = model.fit(X_train,y_train)\n",
    "\n",
    "    print('Train:', model.score(X_train,y_train))\n",
    "    print('Test: ',  model.score(X_test,y_test))\n",
    "    return\n",
    "\n",
    "\n",
    "# Simple Model Forms\n",
    "\n",
    "features = ['columns_etc']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "model = Model(**params).fit(Xtrain,ytrain)\n",
    "model.score(Xtrain,ytrain)\n",
    "model.score(Xtest,ytest)\n",
    "\n",
    "#======================================\n",
    "\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, results_dataframe, save = True, \n",
    "              rando_state = 76, is_neural_network = False, \n",
    "              NN_epochs = 10, NN_batch_size = 32, NN_verbose = 0):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = rando_state)\n",
    "\n",
    "model      = model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test  = model.predict(X_test)\n",
    "dict_model = {'model' : re.findall(r'^[^@]+\\(', str(model))[0].strip(\"(\"),\n",
    "         'parameters' : model.get_params()}\n",
    "        \n",
    "\n",
    "    \n",
    "    # Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    #dict_model['train_Median_error'] = metrics.median_absolute_error(y_train, pred_train)\n",
    "    #dict_model['train_R_squared']    = metrics.r2_score(y_train, pred_train)\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    #dict_model['test_Median_error']  = metrics.median_absolute_error(y_test, pred_test)\n",
    "    #dict_model['test_R_squared']     = metrics.r2_score(y_test, pred_test)\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE : ' + str(dict_model['train_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['train_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['train_R_squared']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE : ' + str(dict_model['test_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['test_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['test_R_squared']))\n",
    "    \n",
    "    # Saving current results\n",
    "    results_dataframe = results_dataframe.append(dict_model, ignore_index=True)\n",
    "    if save == True:\n",
    "        results_dataframe.to_csv('./data/modeling_results_' + datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "                          , index = False)\n",
    "    \n",
    "    return results_dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Regressors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "#, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "scaled_df = pd.DataFrame(StandardScaler().fit_transform(df),\n",
    "                         columns = df.columns)\n",
    "scaled_df.head()\n",
    "\n",
    "X = scaled_df.drop(columns = ['target'])\n",
    "y = scaled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "lin_reg    = LinearRegression().fit(X_train, y_train)\n",
    "knn_reg    = KNeighborsRegressor().fit(X_train, y_train)\n",
    "cart_reg   = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "bag_reg    = BaggingRegressor().fit(X_train, y_train)\n",
    "randof_reg = RandomForestRegressor().fit(X_train, y_train)\n",
    "ada_reg    = AdaBoostRegressor().fit(X_train, y_train)\n",
    "SV_reg     = SVR().fit(X_train, y_train)\n",
    "models     = [\n",
    "    lin_reg,\n",
    "    knn_reg,\n",
    "    cart_reg,\n",
    "    bag_reg,\n",
    "    randof_reg,\n",
    "    ada_reg,\n",
    "    SV_reg    \n",
    "]\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Classifications:\n",
    "X = scaled_df.drop(columns = ['e401k', 'p401k'])\n",
    "y = [1 if scaled_df['e401k'][i] > 0 else 0 for i in range(scaled_df.shape[0])]\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "LogisticRegression().fit(X_train, y_train)\n",
    "KNeighborsClassifier().fit(X_train, y_train)\n",
    "DecisionTreeClassifier().fit(X_train, y_train)\n",
    "BaggingClassifier().fit(X_train, y_train)\n",
    "RandomForestClassifier().fit(X_train, y_train)\n",
    "AdaBoostClassifier().fit(X_train, y_train)\n",
    "SVC().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Multi NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "model = nb.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "model.score(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "X = df[[]]\n",
    "y = df[]\n",
    "df_results = run_model(XGBRegressor(objective='reg:squarederror'), X, y, df_results)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "xgb_regressor=XGBRegressor(max_depth=7, \n",
    "                           n_estimators=500, \n",
    "                           objective=\"reg:linear\", \n",
    "                           min_child_weight = 6,\n",
    "                           subsample = 0.87,\n",
    "                           colsample_bytree = 0.50,\n",
    "                           scale_pos_weight = 1.0,                       \n",
    "                           learning_rate=0.1)\n",
    "\n",
    "xgb_regressor.fit(X_train, y_log, eval_metric=RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Needed:\n",
    " 1.) Imports\n",
    " 2.) Classification Metrics\n",
    " #==================================\n",
    "\n",
    "# For general Regression.\n",
    "def RMSE(y_real, y_hat):  # Goal=0, Avg Distance\n",
    "    return np.sqrt(metrics.mean_squared_error(y_real, y_hat))\n",
    "\n",
    "#==================================\n",
    "# For Neural Network\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#====================================\n",
    "# All Other Important Regression Metrics\n",
    "\n",
    "def Reg_metrics(y_real, y_hat):\n",
    "    print('RMSE            :',np.sqrt(metrics.mean_squared_error(y_real, y_hat)))   # Goal=0, Avg Distance\n",
    "    print('Median Abs Error:',metrics.median_absolute_error(y_real,y_hat))          # Goal=0, Median Distance\n",
    "    print('R Squared       :',metrics.r2_score(y_real, y_hat))          # Goal=1, Percent model can explain\n",
    "    return\n",
    "\n",
    "#====================================\n",
    "\n",
    "\n",
    "def rmse_score(model, \n",
    "               X_train = X_train,\n",
    "               X_test = X_test,\n",
    "               y_train = y_train,\n",
    "               y_test = y_test):\n",
    "    train_score = mean_squared_error(y_true = y_train,\n",
    "                                     y_pred = model.predict(X_train)) ** 0.5\n",
    "    test_score = mean_squared_error(y_true = y_test,\n",
    "                                    y_pred = model.predict(X_test)) ** 0.5\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return\n",
    "\n",
    "#====================================\n",
    "def f1_score(model, \n",
    "               X_train = Xr_train,\n",
    "               X_test = Xr_test,\n",
    "               y_train = yr_train,\n",
    "               y_test = yr_test):\n",
    "    train_score = f1_score(y_train,\n",
    "                           model.predict(X_train))\n",
    "    test_score  = f1_score(y_test,\n",
    "                           model.predict(X_test))\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return\n",
    "\n",
    "#====================================\n",
    "def run_model(model, X_train, X_test, y_train, y_test, results_dataframe, save = True, \n",
    "              rando_state = 76, is_neural_network = False, \n",
    "              NN_epochs = 10, NN_batch_size = 32, NN_verbose = 0):\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = rando_state)\n",
    "\n",
    "model      = model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test  = model.predict(X_test)\n",
    "dict_model = {'model' : re.findall(r'^[^@]+\\(', str(model))[0].strip(\"(\"),\n",
    "         'parameters' : model.get_params()}\n",
    "        \n",
    "\n",
    "    \n",
    "    # Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE : ' + str(dict_model['train_RMSE']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE : ' + str(dict_model['test_RMSE']))\n",
    "    \n",
    "    # Saving current results\n",
    "    results_dataframe = results_dataframe.append(dict_model, ignore_index=True)\n",
    "    if save == True:\n",
    "        results_dataframe.to_csv('./data/modeling_results_' + datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "                          , index = False)\n",
    "    \n",
    "    return results_dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#====================================\n",
    "\n",
    "# Alternmative Measures for above Function\n",
    "# Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    #dict_model['train_Median_error'] = metrics.median_absolute_error(y_train, pred_train)\n",
    "    #dict_model['train_R_squared']    = metrics.r2_score(y_train, pred_train)\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    #dict_model['test_Median_error']  = metrics.median_absolute_error(y_test, pred_test)\n",
    "    #dict_model['test_R_squared']     = metrics.r2_score(y_test, pred_test)\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE : ' + str(dict_model['train_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['train_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['train_R_squared']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE : ' + str(dict_model['test_RMSE']))\n",
    "    #print('Median Abs Error : ' + str(dict_model['test_Median_error']))\n",
    "    #print('R Squared        : ' + str(dict_model['test_R_squared']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Unsupervised Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(the_X,the_kmodel.labels_) # -1 = Bad 0 = Meh 1 = Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "# DBScan\n",
    "#model = DBSCAN(eps = 3, min_samples = 3)\n",
    "#test_df['y_hat'] = model.fit_predict(X)\n",
    "\n",
    "# K Means\n",
    "#k_model = KMeans(n_clusters=3)\n",
    "#model = k_model.fit(df[['x_axis', 'y_axis']])\n",
    "\n",
    "# Visualizing based on 2 columns\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.scatter(the_df['x'],the_df['y'], c = the_df['DBS_y'], alpha=0.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(the_kmodel.cluster_centers_, columns = ['x_axis', 'y_axis'])\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clusters(df,\n",
    "                     n_clusters_kmeans  = 3,\n",
    "                     n_clusters_agg     = 3,\n",
    "                     eps_dbscan         = 3,\n",
    "                     min_samples_dbscan = 3):\n",
    "    \n",
    "    fig,ax  = plt.subplots(nrows = 1, ncols = 4, figsize=(24, 7)) # Init graph backgrounds & their positions\n",
    "    kmeans  = KMeans(n_clusters = n_clusters_kmeans)\n",
    "    agclus  = AgglomerativeClustering(n_clusters = n_clusters_agg)\n",
    "    dbscan  = DBSCAN(eps = eps_dbscan, min_samples = min_samples_dbscan)\n",
    "    model_tuples = [(False,  0, 'label'),\n",
    "                    (kmeans, 1, 'K_Means'), \n",
    "                    (agclus, 2, 'Agg_clust'), \n",
    "                    (dbscan, 3, 'DBScan')]\n",
    "\n",
    "    for tup in model_tuples:\n",
    "        # [0] = Model\n",
    "        # [1] = Axis Number\n",
    "        # [2] = label string\n",
    "        if tup[0] == False:\n",
    "            pass\n",
    "        else:\n",
    "            model = tup[0].fit(df.iloc[:,0:2])\n",
    "            df[tup[2]] = model.labels_\n",
    "        colors = plt.cm.Paired(np.linspace(0, 1, len(df[tup[2]].unique())))\n",
    "        for label, color in zip(df[tup[2]].unique(), colors):\n",
    "            X_  = df[df[tup[2]] == label]\n",
    "            ax[tup[1]].scatter(X_.iloc[:, 0], X_.iloc[:, 1], s = 70,\n",
    "                            color = color, label = label, alpha = .9)\n",
    "\n",
    "        ax[tup[1]].set_title(tup[2], fontsize=30, color = 'Black')\n",
    "        ax[tup[1]].legend(loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['model', 'features', 'parameters',\n",
    "    'train_RMSE', 'train_Median_error', 'train_R_squared',\n",
    "    'test_RMSE',  'test_Median_error',  'test_R_squared', 'time'])\n",
    "\n",
    "def run_model(model, X, y, results_dataframe, save = True, \n",
    "              rando_state = 76, is_neural_network = False, \n",
    "              NN_epochs = 10, NN_batch_size = 32, NN_verbose = 0):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = rando_state)\n",
    "\n",
    "    if is_neural_network == False:\n",
    "        # Performing normal running of model.\n",
    "        model      = model.fit(X_train, y_train)\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_test  = model.predict(X_test)\n",
    "        dict_model = {'model' : re.findall(r'^[^@]+\\(', str(model))[0].strip(\"(\"),\n",
    "                 'parameters' : model.get_params()}\n",
    "        \n",
    "    elif is_neural_network == True:\n",
    "        # Running model for Neural Networks\n",
    "        results = model.fit(X_train, y_train, \n",
    "                    epochs= NN_epochs, \n",
    "                    batch_size = NN_batch_size,\n",
    "                    verbose = NN_verbose,\n",
    "                    validation_data = (X_test,y_test))\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_test  = model.predict(X_test)\n",
    "        dict_model = {'model' : \"Neural Network\",\n",
    "                 'parameters' : {'layers' : [layer.get_config()['units'] for layer in results.model.layers],\n",
    "                                 'batch_size' : NN_batch_size,\n",
    "                                 'epochs'     : NN_epochs}}\n",
    "\n",
    "    else:\n",
    "        print(\"A non-boolian value was passed to is_neural_network.  This is an error.\")\n",
    "        return\n",
    "    \n",
    "    # Adding non-model dependant information to dict_model\n",
    "    dict_model['time']               = datetime.fromtimestamp(time()).strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    dict_model['features']           = [feature for feature in X.columns]\n",
    "    dict_model['train_RMSE']         = np.sqrt(metrics.mean_squared_error(y_train, pred_train))\n",
    "    dict_model['train_Median_error'] = metrics.median_absolute_error(y_train, pred_train)\n",
    "    dict_model['train_R_squared']    = metrics.r2_score(y_train, pred_train)\n",
    "    dict_model['test_RMSE']          = np.sqrt(metrics.mean_squared_error(y_test, pred_test))\n",
    "    dict_model['test_Median_error']  = metrics.median_absolute_error(y_test, pred_test)\n",
    "    dict_model['test_R_squared']     = metrics.r2_score(y_test, pred_test)\n",
    "    \n",
    "    # Printing current results\n",
    "    print(dict_model['model'] + ' Train')\n",
    "    print('RMSE             : ' + str(dict_model['train_RMSE']))\n",
    "    print('Median Abs Error : ' + str(dict_model['train_Median_error']))\n",
    "    print('R Squared        : ' + str(dict_model['train_R_squared']))\n",
    "    print('\\n' + dict_model['model'] + ' Test')\n",
    "    print('RMSE             : ' + str(dict_model['test_RMSE']))\n",
    "    print('Median Abs Error : ' + str(dict_model['test_Median_error']))\n",
    "    print('R Squared        : ' + str(dict_model['test_R_squared']))\n",
    "    \n",
    "    # Saving current results\n",
    "    results_dataframe = results_dataframe.append(dict_model, ignore_index=True)\n",
    "    if save == True:\n",
    "        results_dataframe.to_csv('./data/modeling_results_' + datetime.fromtimestamp(time()).strftime('%m_%d_%Y')\n",
    "                          , index = False)\n",
    "    \n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[features]]\n",
    "y = df[target]\n",
    "df_results = run_model(LogisticRegression(), X, y, df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = run_model(KNeighborsClassifier(), X, y, df_results)\n",
    "df_results = run_model(DecisionTreeClassifier(), X, y, df_results)\n",
    "df_results = run_model(RandomForestClassifier(), X, y, df_results)\n",
    "df_results = run_model(AdaBoostClassifier(), X, y, df_results)\n",
    "df_results = run_model(ExtraTreesClassifier(), X, y, df_results)\n",
    "df_results = run_model(GradientBoostingClassifier(), X, y, df_results)\n",
    "df_results = run_model(SVR(), X, y, df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'param':[list_of_options]\n",
    "}\n",
    "Model_gridsearch = GridSearchCV(Model(), params, cv=5, verbose=1, n_jobs=2,)\n",
    "\n",
    "\n",
    "df_results = run_model(Model_gridsearch, X, y, df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
